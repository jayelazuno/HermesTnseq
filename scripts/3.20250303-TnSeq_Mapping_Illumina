#!/usr/bin/env python3

from collections import Counter
import sys
import subprocess
import numpy as np
import random

"""
updated 01/12/2026
TnSeq MAPPING (MAIN) + IN-RUN QC/CLEANING (TIDY)
- Map reads (bowtie2) to Forward index (and Reverse index too if PE)
- Create ONE BAM output (already QC-filtered), sort + index
- Call insertion sites from the QC BAM (strand-aware 5' base)
- Write SiteCount.csv, BED6 (IGV), bedGraph + bigWig (raw + CPM)
- Map insertion sites to genes (GeneCount.csv)
- MidLc complexity

Key conventions:
- Insertion site = 5' coordinate of alignment
  + strand: site = start (0-based)
  - strand: site = end-1 (0-based)
- BED is 0-based, half-open [start, end)
"""

# ------------------------------ helpers ---------------------------------

def run(cmd: str):
    subprocess.run(cmd, shell=True, check=True)

def die(msg: str):
    print(f"ERROR: {msg}", file=sys.stderr)
    sys.exit(1)

def parse_int(s, default=None):
    try:
        return int(s)
    except Exception:
        return default

# ------------------------------ inputs ----------------------------------

GenomicFiles = input("What is the location (pathname) of the genomic files folder: ").strip()
ForwardIndexForBowtie = GenomicFiles + "/Forward/Forward"
ReverseIndexForBowtie = GenomicFiles + "/Reverse/Reverse"

AnnotationFile = input("What is the location (pathname) of the Annotation File: ").strip()

read_type = input("Is this paired-end (PE) or single-end (SE) data? (PE/SE): ").strip().upper()
if read_type not in ("PE", "SE"):
    die("Invalid input. Please enter either PE or SE.")

if read_type == "PE":
    ForwardReadFastQ = input("What is the Forward Read file pathname (FastQ or FastQ Zip): ").strip()
    ReverseReadFastQ = input("What is the Reverse Read file pathname (FastQ or FastQ Zip): ").strip()
else:
    ForwardReadFastQ = input("What is the Read file pathname (FastQ or FastQ Zip): ").strip()
    ReverseReadFastQ = None

Prefix_For_File_Names = input("What is the prefix wanted for your file names (String with no spaces): ").strip()
if not Prefix_For_File_Names:
    die("Prefix cannot be empty.")

# ------------------------------ outputs ----------------------------------

ForwardMappingSam = Prefix_For_File_Names + "F.sam"
ReverseMappingSam = Prefix_For_File_Names + "R.sam" if read_type == "PE" else None

SiteCountFileName = Prefix_For_File_Names + "SiteCount.csv"
GeneCountFileName = Prefix_For_File_Names + "GeneCount.csv"
removedGeneCountFileName = "removed" + GeneCountFileName

BedFileName = Prefix_For_File_Names + ".bed"  # BED6 for IGV
BedFileIdentifier = Prefix_For_File_Names

# ONE QC BAM output
BamFileName = Prefix_For_File_Names + ".bam"
sortedBamFileName = "sorted." + BamFileName

BedGraphName = Prefix_For_File_Names + "_bg.bedgraph"
deeptoolsBigWigName = Prefix_For_File_Names + "_deeptools.bw"
normalizedBigWigName = "normalized" + deeptoolsBigWigName

MidLcFileName = Prefix_For_File_Names + "MidLc.csv"

# ------------------------------ options ----------------------------------

perform_trimming = input("Do you need to perform trimming (Y or N)?: ").strip().upper()
if perform_trimming == "Y":
    fiveprimetrimarea = float(input("Five Prime Percentage to Discount for Mapping (Decimal Format): ").strip())
    threeprimetrimarea = float(input("Three Prime Percentage to Discount for Mapping (Decimal Format): ").strip())
else:
    fiveprimetrimarea = 0.0
    threeprimetrimarea = 0.0

num_cores = int((input("Enter the number of cores to use for parallel processing (e.g., 4): ").strip() or "1"))
MAPQ_CUTOFF = int((input("MAPQ cutoff for BAM (e.g., 20): ").strip() or "20"))

# Drop: unmapped(0x4), secondary(0x100), duplicate(0x400), supplementary(0x800)
DROP_FLAGS = "0xD04"

# ------------------------------ mapping ----------------------------------

if read_type == "PE":
    run(f'bowtie2 -x "{ForwardIndexForBowtie}" -1 "{ForwardReadFastQ}" -2 "{ReverseReadFastQ}" -S "{ForwardMappingSam}" -p {num_cores}')
    run(f'bowtie2 -x "{ReverseIndexForBowtie}" -1 "{ForwardReadFastQ}" -2 "{ReverseReadFastQ}" -S "{ReverseMappingSam}" -p {num_cores}')
else:
    run(f'bowtie2 -x "{ForwardIndexForBowtie}" -U "{ForwardReadFastQ}" -S "{ForwardMappingSam}" -p {num_cores}')

# SAM -> QC BAM -> sort/index (single BAM output)
run(f'samtools view -@ {num_cores} -b -q {MAPQ_CUTOFF} -F {DROP_FLAGS} "{ForwardMappingSam}" > "{BamFileName}"')
run(f'samtools sort -@ {num_cores} "{BamFileName}" -o "{sortedBamFileName}"')
run(f'samtools index -@ {num_cores} "{sortedBamFileName}"')

# ------------------------------ insertion sites --------------------------

sites_bed3 = Prefix_For_File_Names + ".sites.bed3"
sites_bg = Prefix_For_File_Names + ".sites.bedgraph4"  # chrom start end count

cmd_sites_bed3 = (
    f'bedtools bamtobed -i "{sortedBamFileName}" '
    f'| awk \'BEGIN{{OFS="\\t"}}{{ if($6=="+") print $1,$2,$2+1; else print $1,$3-1,$3 }}\' '
    f'| sort -k1,1 -k2,2n -k3,3n | uniq > "{sites_bed3}"'
)
run(cmd_sites_bed3)

cmd_sites_bg = (
    f'bedtools bamtobed -i "{sortedBamFileName}" '
    f'| awk \'BEGIN{{OFS="\\t"}}{{ if($6=="+") print $1,$2,$2+1; else print $1,$3-1,$3 }}\' '
    f'| sort -k1,1 -k2,2n -k3,3n '
    f'| bedtools groupby -g 1,2,3 -c 1 -o count > "{sites_bg}"'
)
run(cmd_sites_bg)

# ------------------------------ SiteCount.csv + BED6 ---------------------
# FIX: aggregate by (chrom, site, orient); do NOT include count in the Counter key.

site_counts = Counter()  # (chrom, site_1based, "F") -> total_reads

with open(sites_bg, "r") as f:
    for line in f:
        chrom, start0, end0, count = line.rstrip("\n").split("\t")
        site_1based = int(start0) + 1
        site_counts[(chrom, site_1based, "F")] += int(count)

with open(SiteCountFileName, "w") as out:
    for (chrom, site_1based, orient), total in site_counts.items():
        out.write(f"{chrom}, {site_1based}, {orient}, {total}\n")

with open(BedFileName, "w") as out:
    out.write(f"track name={BedFileIdentifier} useScore=1\n")
    for (chrom, site_1based, orient), total in site_counts.items():
        start0 = site_1based - 1
        end0 = site_1based

        score = total * 20 + 100
        if score > 1000:
            score = 1000

        strand = "+" if orient == "F" else "-"
        out.write(f"{chrom}\t{start0}\t{end0}\t.\t{score}\t{strand}\n")

# ------------------------------ GeneCount --------------------------------

features_by_chrom = {}
MappedSitesData = {}

with open(AnnotationFile, "r") as features:
    for line in features:
        if not line.strip() or line.startswith("#"):
            continue
        cols = line.split()
        if len(cols) < 7:
            continue

        Chrom = cols[0]
        genename = cols[1]
        featuretype = cols[2]

        try:
            p1 = int(cols[3])
            p2 = int(cols[4])
        except Exception:
            continue

        startfeature = min(p1, p2)
        endfeature = max(p1, p2)
        orientation = cols[6]

        Gene_Total_Identifier = genename + "," + featuretype
        MappedSitesData.setdefault(Gene_Total_Identifier, []).append(0)

        features_by_chrom.setdefault(Chrom, []).append(
            (startfeature, endfeature, orientation, Gene_Total_Identifier)
        )

for (Site_Chrom, Site_Location, _orient), Site_Counts in site_counts.items():
    feats = features_by_chrom.get(Site_Chrom, [])
    for (Feature_Start, Feature_End, Feature_Orientation, Feature_Name) in feats:
        gene_len = abs(Feature_End - Feature_Start)
        Trim_End_Size = threeprimetrimarea * gene_len
        Trim_Start_Size = fiveprimetrimarea * gene_len

        fs = Feature_Start
        fe = Feature_End

        # 3'
        if Feature_Orientation == "-":
            fs = int(fs + Trim_End_Size)
        if Feature_Orientation == "+":
            fe = int(fe - Trim_End_Size)

        # 5'
        if Feature_Orientation == "-":
            fe = int(fe - Trim_Start_Size)
        if Feature_Orientation == "+":
            fs = int(fs + Trim_Start_Size)

        if fs <= Site_Location <= fe:
            MappedSitesData.setdefault(Feature_Name, []).append(str(Site_Counts))

with open(GeneCountFileName, "w") as genecsv, open(removedGeneCountFileName, "w") as genecsvremoved:
    for key, value in MappedSitesData.items():
        newlist = [int(x) for x in value]
        newlistTopRemoved = list(newlist)

        if len(newlist) == 0:
            continue

        newlist.sort(reverse=True)
        newlistTopRemoved.sort(reverse=True)

        if len(newlist) == 1:
            newlist.append(0)
        if len(newlistTopRemoved) == 1:
            newlistTopRemoved.append(0)

        del newlistTopRemoved[0]

        genecsv.write(f"{key} , {sum(newlist)}\n")
        genecsvremoved.write(f"{key} , {sum(newlistTopRemoved)}\n")

# ------------------------------ Coverage tracks --------------------------

run(f'bedtools genomecov -bg -ibam "{sortedBamFileName}" > "{BedGraphName}"')
run(f'bamCoverage -b "{sortedBamFileName}" -o "{deeptoolsBigWigName}"')
run(f'bamCoverage -b "{sortedBamFileName}" --normalizeUsing CPM -o "{normalizedBigWigName}"')

# ------------------------------ MidLc ------------------------------------

alllines = []
for (chrom, site_1based, orient), total in site_counts.items():
    toadd = f"{chrom}{site_1based}{orient}"
    for _ in range(total):
        alllines.append(toadd)

maxreads = len(alllines)

with open(MidLcFileName, "w") as out:
    out.write("Reads Sampled,Unique Sites Trial1,Unique Sites Trial2,Unique Sites Trial3\n")
    if maxreads == 0:
        out.write("0,0,0,0\n")
    else:
        numberofrandomsamples = 100
        while maxreads >= numberofrandomsamples:
            t = []
            for _ in range(3):
                choice = random.sample(alllines, numberofrandomsamples)
                t.append(len(set(choice)))
            out.write(f"{numberofrandomsamples},{t[0]},{t[1]},{t[2]}\n")
            numberofrandomsamples = int(round(numberofrandomsamples * 4))

        t = []
        for _ in range(3):
            choice = random.sample(alllines, maxreads)
            t.append(len(set(choice)))
        out.write(f"{maxreads},{t[0]},{t[1]},{t[2]}\n")
