
#!/usr/bin/env python

from collections import Counter, OrderedDict, defaultdict
import csv
import os
import operator
import json
import numpy as np
import random

### New code for main running; need to add genome loop from processing code instead of by file
### Work into the genomefiles page

########
"""
Input and process all of the information for file input and output
"""
########

GenomicFiles = input("What is the location (pathname) of the genomic files folder: ")

ForwardIndexForBowtie = GenomicFiles + "/Forward/Forward"
ReverseIndexForBowtie = GenomicFiles + "/Reverse/Reverse"
GenomeSizeFile = GenomicFiles + "/sizes.genome"

AnnotationFile = input("What is the location (pathname) of the Annotation File: ")

# Ask whether the data is paired-end or single-end
read_type = input("Is this paired-end (PE) or single-end (SE) data? (PE/SE): ").strip().upper()

if read_type == "PE":
    ForwardReadFastQ = input("What is the Forward Read file pathname (FastQ or FastQ Zip): ")
    ReverseReadFastQ = input("What is the Reverse Read file pathname (FastQ or FastQ Zip): ")
    Demultiplexed_name_Reverse = None  # Will be set later
elif read_type == "SE":
    ForwardReadFastQ = input("What is the Read file pathname (FastQ or FastQ Zip): ")
    ReverseReadFastQ = None
else:
    print("Invalid input. Please enter either PE or SE.")
    exit(1)

Prefix_For_File_Names = input("What is the prefix wanted for your file names (String with no spaces): ")

Demultiplexed_name_Forward = Prefix_For_File_Names + "F.fastq"
ForwardMappingSam = Prefix_For_File_Names + "F.sam"
SiteCountFileName = Prefix_For_File_Names + "SiteCount.csv"
GeneCountFileName = Prefix_For_File_Names + "GeneCount.csv"
removedGeneCountFileName = "removed" + GeneCountFileName
BedFileName = Prefix_For_File_Names + ".bed"
BedFileIdentifier = Prefix_For_File_Names
BamFileName = Prefix_For_File_Names + ".bam"
sortedBamFileName = "sorted." + BamFileName
MidLcFileName = Prefix_For_File_Names + "MidLc.csv"
BedGraphName = Prefix_For_File_Names + "_bg.bedgraph"
deeptoolsBedGraphName = Prefix_For_File_Names + "_deeptools.bw"
normalizedBedGraphName = "normalized" + deeptoolsBedGraphName

# Only set reverse files if paired-end
if read_type == "PE":
    Demultiplexed_name_Reverse = Prefix_For_File_Names + "R.fastq"
    ReverseMappingSam = Prefix_For_File_Names + "R.sam"

########
"""
Ask if indexing, trimming, and demultiplexing are needed
"""
########

perform_indexing = input("Do you need to perform indexing (Y or N)?: ").strip().upper()
perform_trimming = input("Do you need to perform trimming (Y or N)?: ").strip().upper()

# Only ask about demultiplexing if paired-end
if read_type == "PE":
    perform_demultiplexing = input("Do you need to perform demultiplexing (Y or N)?: ").strip().upper()
else:
    perform_demultiplexing = "N"

########
"""
Initialize the index number and sequence(s) if needed
"""
########

if perform_demultiplexing == "Y" and read_type == "PE":
    Index_List_File_Name = input("What is the location of the index list?: ")

    NumberOfIndexes = int(input("Enter Index Count: "))

    index_file = open(Index_List_File_Name, "r")
    dict_of_indexes = {}
    i = 1
    for line in index_file:
        line = line.strip()
        dict_of_indexes[i] = line
        i += 1

    CurrentIndexEntered = 0
    IndexString = ""
    while NumberOfIndexes != CurrentIndexEntered:
        CurrentIndexEntered += 1
        IndexStringID = int(input("Enter Index " + str(CurrentIndexEntered) + " as the index id: "))
        TempIndexString = "-g ^" + str(dict_of_indexes[IndexStringID])
        IndexString = IndexString + " " + (TempIndexString)

########
"""
Any trimming of gene length/locations if needed
"""
########

if perform_trimming == "Y":
    fiveprimetrimarea = float(input("Five Prime Percentage to Discount for Mapping (Decimal Format): "))
    threeprimetrimarea = float(input("Three Prime Percentage to Discount for Mapping (Decimal Format): "))
else:
    fiveprimetrimarea = 0
    threeprimetrimarea = 0

########
"""
Demultiplexing the runs using Cutadapt if needed
"""
########

if perform_demultiplexing == "Y" and read_type == "PE":
    print("Final command: " + "cutadapt " + str(IndexString) + " --discard-untrimmed -o " + Demultiplexed_name_Reverse + " -p " + Demultiplexed_name_Forward + " " + ReverseReadFastQ + " " + ForwardReadFastQ)
    os.system("cutadapt " + str(IndexString) + " --discard-untrimmed -o " + Demultiplexed_name_Reverse + " -p " + Demultiplexed_name_Forward + " " + ReverseReadFastQ + " " + ForwardReadFastQ)
elif read_type == "PE":
    # If no demultiplexing but paired-end, use the original FastQ files directly
    Demultiplexed_name_Forward = ForwardReadFastQ
    Demultiplexed_name_Reverse = ReverseReadFastQ
else:
    # Single-end data
    Demultiplexed_name_Forward = ForwardReadFastQ

########
"""
Running bowtie2 with user-specified number of cores
"""
########

# Add this line to request the number of cores
num_cores = input("Enter the number of cores to use for parallel processing (e.g., 4): ")
num_cores = int(num_cores)  # Convert the input to an integer

# Modify the bowtie2 commands based on read type
if read_type == "PE":
    os.system(f"bowtie2 -x {ForwardIndexForBowtie} -1 {Demultiplexed_name_Forward} -2 {Demultiplexed_name_Reverse} -S {ForwardMappingSam} -p {num_cores}")
    os.system(f"bowtie2 -x {ReverseIndexForBowtie} -1 {Demultiplexed_name_Forward} -2 {Demultiplexed_name_Reverse} -S {ReverseMappingSam} -p {num_cores}")
else:
    os.system(f"bowtie2 -x {ForwardIndexForBowtie} -U {Demultiplexed_name_Forward} -S {ForwardMappingSam} -p {num_cores}")
    # For single-end, we don't process reverse reads
    ReverseMappingSam = None

########
"""
Convert sam file to SiteCount file
"""
########

All_Sites_List = []

with open(ForwardMappingSam, "r") as sam:
    for line in sam:
        # Skip header lines (lines starting with "@")
        if line.startswith("@"):
            continue

        # Split the line into fields
        fields = line.split()

        # Extract relevant fields
        qscore = int(fields[4])  # Mapping quality (5th column)
        chrom = fields[2]        # Chromosome (3rd column)
        flag = int(fields[1])    # Strand (2nd column, flag)
        start = fields[3]        # Start position (4th column)

        # Filter by quality score
        if qscore > 20:
            # Check if the alignment is on the forward strand (flag == 0)
            if flag == 0:
                # Extract the MD tag (if present)
                md_tag = None
                for field in fields:
                    if field.startswith("MD:Z:"):
                        md_tag = field
                        break

                # Skip alignments with MD:Z:0 (if desired)
                if md_tag != "MD:Z:0":
                    keytotal = f"{chrom}, {start}, F"
                    All_Sites_List.append(keytotal)

### Processing Reverse Reads (only for PE data)
if read_type == "PE":
    chromosome_sizes = {}
    GenomeSizeFileToLoop = open(GenomeSizeFile, "r")
    for line in GenomeSizeFileToLoop:
        sizechrom = line.split()[0]
        sizechrom = sizechrom.strip()
        size = line.split()[1]
        size = size.strip()
        chromosome_sizes[sizechrom] = size

    sam = open(ReverseMappingSam, "r")
    for line in sam:
        if "M0" in line:
            qscore = int(line.split()[4])
            if qscore > 20:
                CorrectChrom = line.split()[2]  # is it in chrom
                direction = line.split()[1]
                startlocation = line.split()[3]  # where it is mapping to start
                MDtag = line.split()[17]
                if direction == "0" and "MD:Z:0" not in MDtag:
                    for chromosome_key, chromosome_value in chromosome_sizes.items():
                        chromosome_key_raw = chromosome_key.strip()
                        if chromosome_key_raw == CorrectChrom:
                            CorrectLocation = str(abs(int(chromosome_value) - int(startlocation)) + 1)
                            keytotal = CorrectChrom + ", " + CorrectLocation + ", R"
                            All_Sites_List.append(keytotal)

Counted_All_Sites_List = Counter(All_Sites_List)

output = open(SiteCountFileName, "w")
for key, value in Counted_All_Sites_List.items():
    key = str(key)
    value = str(value)
    output.write(key + "," + value + "\n")

###
"""
Convert Site Count file to Bed file
"""
###

sitecountfile = open(SiteCountFileName, "r")

output = open(BedFileName, "w")
output.write("track name=" + BedFileIdentifier + " useScore=1\n")
for line in sitecountfile:
    if "mito" not in line:
        chrom = line.split(",")[0]
        start = line.split(",")[1]
        count = line.split(",")[3]
        startprime = int(start)
        start = str(start)
        count = count.strip()
        countprime = int(count)
        count = str(countprime * 20 + 100)
        strand = line.split(",")[2]
        strand = strand.rstrip()
        if strand == " F":
            startplus1 = startprime + 1
            startplus1 = str(startplus1)
            strand = "+"
            output.write(chrom + "\t" + start + "\t" + start + "\t" + "." + "\t" + count + "\t" + strand + "\n")
        if strand == " R":
            startminus1 = startprime - 1
            startminus1 = str(startminus1)
            strand = "-"
            output.write(chrom + "\t" + start + "\t" + start + "\t" + "." + "\t" + count + "\t" + strand + "\n")

#####
"""
Converting SiteCount.csv into GeneCount.csv
"""
###

All_Features = {}
All_Features_list = []
MappedSitesData = {}
Count_Feature = 0
features = open(AnnotationFile, "r")
for line in features:
    if "#" not in line:  # make sure it is not the header
        featuretype = line.split()[2]
        Chrom = line.split()[0]  # what chromosome
        featuretype = line.split()[2]  # what type of feature
        StorePositions = []
        StorePositions.append(int(line.split()[3]))
        StorePositions.append(int(line.split()[4]))
        StorePositions.sort()
        startfeature = str(StorePositions[0])
        endfeature = str(StorePositions[1])

        orientation = line.split()[6]  # orientation of the gene
        genename = line.split()[1]  # gene name from second column (make sure this is standard name)
        Count_Feature += 1
        Gene_Total_Identifier = genename + "," + featuretype
        MappedSitesData.setdefault(Gene_Total_Identifier, []).append(0)

        All_Features.update({Count_Feature: [startfeature, endfeature, orientation, Gene_Total_Identifier, Chrom, featuretype]})

"""Mapping and output as a gene count file"""

sitecountfile = open(SiteCountFileName, "r")
for line in sitecountfile:
    Site_Chrom = line.split(",")[0]  # is it in chrom
    Site_Location = line.split(",")[1]  # where it is mapping to start
    Site_Counts = line.split(",")[3]  # number of reads at that site
    Site_Chrom = str(Site_Chrom)
    Site_Location = int(Site_Location)
    Site_Counts = int(Site_Counts)
    for i in All_Features:
        FeatureChrom = All_Features[i][4]
        if FeatureChrom == Site_Chrom:
            Feature_Start = int(All_Features[i][0])  # start of feature
            Feature_End = int(All_Features[i][1])  # end of feature
            Feature_Name = All_Features[i][3]  # feature name
            Feature_Orientation = All_Features[i][2]  # orientation
            Trim_End_Size = threeprimetrimarea * abs(Feature_Start - Feature_End)  # if trimming
            Trim_Start_Size = fiveprimetrimarea * abs(Feature_Start - Feature_End)  # if trimming

            # 3'
            if Feature_Orientation == "-":
                Feature_Start = Feature_Start + Trim_End_Size
            if Feature_Orientation == "+":
                Feature_End = Feature_End - Trim_End_Size

            # 5'
            if Feature_Orientation == "-":
                Feature_End = Feature_End - Trim_Start_Size
            if Feature_Orientation == "+":
                Feature_Start = Feature_Start + Trim_Start_Size

            if Feature_Start <= Site_Location and Site_Location <= Feature_End:  # checks if it is between that gene
                identifier = Feature_Name
                Site_Counts = str(Site_Counts)
                MappedSitesData.setdefault(identifier, []).append(Site_Counts)

genecsv = open(GeneCountFileName, "w")
genecsvremoved = open(removedGeneCountFileName, "w")
for key, value in MappedSitesData.items():
    newlist = []
    newlistTopRemoved = []
    for i in value:
        i = int(i)
        newlist.append(i)
        newlistTopRemoved.append(i)
    maxvalue = max(newlist)
    totalreadsbeforetrim = sum(newlist)
    totalreadsbeforetrim = str(totalreadsbeforetrim)
    totalreadsbeforetrimremoved = sum(newlistTopRemoved)
    totalreadsbeforetrimremoved = str(totalreadsbeforetrimremoved)
    newlist.sort(reverse=True)
    newlistTopRemoved.sort(reverse=True)
    if len(newlist) == 1:
        newlist.append(0)
    if len(newlistTopRemoved) == 1:
        newlistTopRemoved.append(0)
    del newlistTopRemoved[0]  # remove top
    median = np.median(newlist)
    median = str(median)
    totalreads = sum(newlist)
    totalreads = str(totalreads)
    totalreadsremoved = sum(newlistTopRemoved)
    totalreadsremoved = str(totalreadsremoved)
    genecsv.write(key + " , " + totalreads + "\n")
    genecsvremoved.write(key + " , " + totalreadsremoved + "\n")

###
"""
Commands for bedgraph generation
"""
###

os.system("samtools view -S -b " + ForwardMappingSam + " > " + BamFileName)
os.system("samtools sort " + BamFileName + " -o " + "sorted." + BamFileName)
os.system("samtools index " + sortedBamFileName)
os.system("bedtools genomecov -bg -ibam " + sortedBamFileName + " > " + BedGraphName)
os.system("bamCoverage -b " + sortedBamFileName + " -o " + deeptoolsBedGraphName)
os.system("bamCoverage -b " + sortedBamFileName + " --normalizeUsing CPM -o " + normalizedBedGraphName)

###
"""
Calculating MidLc
"""
###

sitecountfile = open(SiteCountFileName)
alllines = []
for line in sitecountfile:
    chrom = line.split(",")[0]
    pos = line.split(",")[1]
    orient = line.split(",")[2]
    counts = int(line.split(",")[3])
    toadd = chrom + pos + orient
    cycles = 0
    while cycles != counts:
        alllines.append(toadd)
        cycles += 1
maxreads = len(alllines)
print(maxreads)
numberofrandomsamples = 100
output = open(MidLcFileName, "w")
output.write("Reads Sampled,Unique Sites Trial1,Unique Sites Trial2,Unique Sites Trial3\n")

while maxreads >= numberofrandomsamples:
    random_choice = random.sample(alllines, numberofrandomsamples)
    random_choice = list(set(random_choice))
    trial1 = len(random_choice)
    trial1 = str(trial1)

    random_choice = random.sample(alllines, numberofrandomsamples)
    random_choice = list(set(random_choice))
    trial2 = len(random_choice)
    trial2 = str(trial2)

    random_choice = random.sample(alllines, numberofrandomsamples)
    random_choice = list(set(random_choice))
    trial3 = len(random_choice)
    trial3 = str(trial3)

    numberofrandomsamplesstr = str(numberofrandomsamples)
    newstr = numberofrandomsamplesstr + "," + trial1 + "," + trial2 + "," + trial3
    output.write(newstr + "\n")
    numberofrandomsamples = int(round(numberofrandomsamples * 4))

print(maxreads)
numberofrandomsamples = maxreads
print(numberofrandomsamples)
random_choice = random.sample(alllines, numberofrandomsamples)
random_choice = list(set(random_choice))
trial1 = len(random_choice)
trial1 = str(trial1)

random_choice = random.sample(alllines, numberofrandomsamples)
random_choice = list(set(random_choice))
trial2 = len(random_choice)
trial2 = str(trial2)

random_choice = random.sample(alllines, numberofrandomsamples)
random_choice = list(set(random_choice))
trial3 = len(random_choice)
trial3 = str(trial3)

numberofrandomsamplesstr = str(numberofrandomsamples)
newstr = numberofrandomsamplesstr + "," + trial1 + "," + trial2 + "," + trial3
output.write(newstr + "\n")
print(newstr)

###
"""End code"""
###
